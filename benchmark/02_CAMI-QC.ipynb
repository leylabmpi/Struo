{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Execute QC pipeline on synth data\n",
    "\n",
    "The aim of this notebook is to run the quality control pipeline on the CAMI communities."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The CAMI High-complexity communities can be found [here](https://data.cami-challenge.org/participate), and were downloaded using the java client from the URL `https://openstack.cebitec.uni-bielefeld.de:8080/swift/v1/CAMI_I_TOY_HIGH`. The original files are interleaved `fastq` files; to deinterleave and obtain forward and reverse reads, we used [the following script]( https://gist.github.com/nathanhaigh/3521724)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Init"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-09-11T08:14:15.592454Z",
     "start_time": "2019-09-11T08:14:11.678924Z"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/ebio/abt3_projects/software/miniconda3_gt4.4/envs/py3/lib/python3.6/importlib/_bootstrap.py:219: RuntimeWarning: numpy.dtype size changed, may indicate binary incompatibility. Expected 96, got 88\n",
      "  return f(*args, **kwds)\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Var"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-09-11T08:14:15.600198Z",
     "start_time": "2019-09-11T08:14:15.595163Z"
    }
   },
   "outputs": [],
   "source": [
    "# Dirs\n",
    "work_dir = \"/ebio/abt3_projects/small_projects/jdelacuesta/DBs_benchmark\"\n",
    "sample_folder = os.path.join(work_dir, \"data\", \"cami\")\n",
    "pipeline_folder = os.path.join(work_dir, \"bin/llmgqc\")\n",
    "\n",
    "# Files\n",
    "samples_file_combined = os.path.join(sample_folder, \"samples.txt\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Prepare config file"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Samples file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-05-08T13:22:50.428199Z",
     "start_time": "2019-05-08T13:22:50.378263Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Sample</th>\n",
       "      <th>Run</th>\n",
       "      <th>Lane</th>\n",
       "      <th>Read1</th>\n",
       "      <th>Read2</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>H_S001</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>/ebio/abt3_projects2/databases_no-backup/CAMI/H...</td>\n",
       "      <td>/ebio/abt3_projects2/databases_no-backup/CAMI/H...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>H_S002</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>/ebio/abt3_projects2/databases_no-backup/CAMI/H...</td>\n",
       "      <td>/ebio/abt3_projects2/databases_no-backup/CAMI/H...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>H_S003</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>/ebio/abt3_projects2/databases_no-backup/CAMI/H...</td>\n",
       "      <td>/ebio/abt3_projects2/databases_no-backup/CAMI/H...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>H_S004</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>/ebio/abt3_projects2/databases_no-backup/CAMI/H...</td>\n",
       "      <td>/ebio/abt3_projects2/databases_no-backup/CAMI/H...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>H_S005</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>/ebio/abt3_projects2/databases_no-backup/CAMI/H...</td>\n",
       "      <td>/ebio/abt3_projects2/databases_no-backup/CAMI/H...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   Sample Run Lane                                              Read1  \\\n",
       "0  H_S001   1    1  /ebio/abt3_projects2/databases_no-backup/CAMI/H...   \n",
       "1  H_S002   1    1  /ebio/abt3_projects2/databases_no-backup/CAMI/H...   \n",
       "2  H_S003   1    1  /ebio/abt3_projects2/databases_no-backup/CAMI/H...   \n",
       "3  H_S004   1    1  /ebio/abt3_projects2/databases_no-backup/CAMI/H...   \n",
       "4  H_S005   1    1  /ebio/abt3_projects2/databases_no-backup/CAMI/H...   \n",
       "\n",
       "                                               Read2  \n",
       "0  /ebio/abt3_projects2/databases_no-backup/CAMI/H...  \n",
       "1  /ebio/abt3_projects2/databases_no-backup/CAMI/H...  \n",
       "2  /ebio/abt3_projects2/databases_no-backup/CAMI/H...  \n",
       "3  /ebio/abt3_projects2/databases_no-backup/CAMI/H...  \n",
       "4  /ebio/abt3_projects2/databases_no-backup/CAMI/H...  "
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "H_S001 = [\"H_S001\", \"1\", \"1\", \"/ebio/abt3_projects2/databases_no-backup/CAMI/High_Complexity/H_S001_R1.fq.gz\", \"/ebio/abt3_projects2/databases_no-backup/CAMI/High_Complexity/H_S001_R2.fq.gz\"]\n",
    "H_S002 = [\"H_S002\", \"1\", \"1\", \"/ebio/abt3_projects2/databases_no-backup/CAMI/High_Complexity/H_S002_R1.fq.gz\", \"/ebio/abt3_projects2/databases_no-backup/CAMI/High_Complexity/H_S002_R2.fq.gz\"]\n",
    "H_S003 = [\"H_S003\", \"1\", \"1\", \"/ebio/abt3_projects2/databases_no-backup/CAMI/High_Complexity/H_S003_R1.fq.gz\", \"/ebio/abt3_projects2/databases_no-backup/CAMI/High_Complexity/H_S003_R2.fq.gz\"]\n",
    "H_S004 = [\"H_S004\", \"1\", \"1\", \"/ebio/abt3_projects2/databases_no-backup/CAMI/High_Complexity/H_S004_R1.fq.gz\", \"/ebio/abt3_projects2/databases_no-backup/CAMI/High_Complexity/H_S004_R2.fq.gz\"]\n",
    "H_S005 = [\"H_S005\", \"1\", \"1\", \"/ebio/abt3_projects2/databases_no-backup/CAMI/High_Complexity/H_S005_R1.fq.gz\", \"/ebio/abt3_projects2/databases_no-backup/CAMI/High_Complexity/H_S005_R2.fq.gz\"]\n",
    "\n",
    "cami_files = [H_S001, H_S002, H_S003, H_S004, H_S005]\n",
    "\n",
    "samples_table = pd.DataFrame(cami_files)\n",
    "samples_table.columns = [\"Sample\", \"Run\", \"Lane\", \"Read1\", \"Read2\"]\n",
    "samples_table"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-05-08T13:22:54.244297Z",
     "start_time": "2019-05-08T13:22:54.215855Z"
    }
   },
   "outputs": [],
   "source": [
    "# Write samples file for QC pipeline\n",
    "samples_file = os.path.join(sample_folder, \"samples.txt\")\n",
    "samples_table.to_csv(samples_file, sep=\"\\t\", index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-05-08T13:22:55.028295Z",
     "start_time": "2019-05-08T13:22:54.897965Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "#-- I/O --#\r\n",
      "# table with sample --> read_file information\r\n",
      "samples_file: tests/samples/samples_amy_n6.txt\r\n",
      "\r\n",
      "# output location\r\n",
      "output_dir: tests/output_amy_n6/\r\n",
      "\r\n",
      "# read file path\r\n",
      "# use \"None\" if full file path is included in the samples_file\r\n",
      "read_file_path: None\r\n",
      "\r\n",
      "#-- Software parameters --#\r\n",
      "# Use \"Skip\" to skip any of these steps. If no params for rule, use \"\"\r\n",
      "# `clumpify`: change dupedist if not HiSeq3000/4000 (dupedist=40 for NextSeq, HiSeq2500, and MiSeq)\r\n",
      "params:\r\n",
      "  # read download\r\n",
      "  remote: just_single=False      # If True, just download read1 (if remote file)\r\n",
      "  # validation, conversion, subsampling\r\n",
      "  validate_reads: \"\"\r\n",
      "  convert_fastq_to_1.8: \"\"\r\n",
      "  seqtk_sample: Skip    # Use number to subsample reads (eg., 1000000)\r\n",
      "  fastqc_on_raw: \"\"\r\n",
      "  # de-duplication\r\n",
      "  clumpify: dedupe=t dupedist=2500 optical=t    # this will likely fail for remote (SRA) samples\r\n",
      "  fastqc_on_dedup: \"\"\r\n",
      "  # adapter removal & quality trimming/filtering\r\n",
      "  bbduk: ref=./adapters/bbmap_adapters.fa fastawrap=300 k=23\r\n",
      "  skewer: -x ./adapters/PE_all.fa -n -l 100 -q 25\r\n",
      "  fastqc_on_qual: \"\"\r\n",
      "  # removal of 'contaminant' reads\r\n",
      "  bbmap: minratio=0.9 maxindel=1 bwr=0.16 bw=12 fast minhits=2 qtrim=r trimq=10 untrim idtag printunmappedcount kfilter=25 maxsites=1 k=14 pairlen=1000 rescuedist=1000\r\n",
      "  fastqc_on_filter: \"\"\r\n",
      "  fastqc_on_final: \"\"\r\n",
      "  # taxonomy\r\n",
      "  centrifuge: Skip\r\n",
      "  krona: Skip\r\n",
      "  # coverage\r\n",
      "  nonpareil: -T kmer\r\n",
      "  nonpareil_summary: 1e9   # this is target seq. depth\r\n",
      "  # master \"Skip\": reads combined then called \"final\" reads (skips all QC steps)\r\n",
      "  skip_all_QC: False\r\n",
      "\r\n",
      "#-- Databases --#\r\n",
      "## hg19 = human genome database for filtering out human reads\r\n",
      "filter_db: /ebio/abt3_projects2/databases_no-backup/hg19/hg19\r\n",
      "# centrifuge db\r\n",
      "centrifuge_db: /ebio/abt3_projects2/databases_no-backup/centrifuge/p+h+v\r\n",
      "# krona taxonomy db\r\n",
      "krona_tax_db: /ebio/abt3_projects2/databases_no-backup/krona/taxonomy\r\n",
      "\r\n",
      "\r\n",
      "#-- Snakemake pipeline --#\r\n",
      "## To use /tmp/global2/, see http://ilm.eb.local/user-guide/#Scratch-space-on-_002ftmp_002fglobal2\r\n",
      "pipeline:\r\n",
      "  snakemake_folder: ./\r\n",
      "  script_folder: ./bin/scripts/\r\n",
      "  temp_folder: /tmp/global2/    # your username will be added automatically to this path\r\n",
      "  run_skip_locally: True        # trivial \"skip\" steps run locally (not qsub)"
     ]
    }
   ],
   "source": [
    "config_default = os.path.join(pipeline_folder, 'config.yaml')\n",
    "!cat $config_default"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-05-08T13:22:55.404308Z",
     "start_time": "2019-05-08T13:22:55.390138Z"
    }
   },
   "outputs": [],
   "source": [
    "config_text = \"\"\"#-- I/O --#\n",
    "# table with sample --> read_file information\n",
    "samples_file: {samples_file}\n",
    "\n",
    "# output location\n",
    "output_dir: {output_dir}\n",
    "\n",
    "# read file path\n",
    "# use \"None\" if full file path is included in the samples_file\n",
    "read_file_path: None\n",
    "\n",
    "#-- Software parameters --#\n",
    "# Use \"Skip\" to skip any of these steps. If no params for rule, use \"\"\n",
    "# `clumpify`: change dupedist if not HiSeq3000/4000 (dupedist=40 for NextSeq, HiSeq2500, and MiSeq)\n",
    "params:\n",
    "  # read download\n",
    "  remote: just_single=False      # If True, just download read1 (if remote file)\n",
    "  # validation, conversion, subsampling\n",
    "  validate_reads: Skip # \"\"\n",
    "  convert_fastq_to_1.8: \"\"\n",
    "  seqtk_sample: Skip    # Use number to subsample reads (eg., 1000000)\n",
    "  fastqc_on_raw: \"\"\n",
    "  # de-duplication\n",
    "  clumpify: Skip # dedupe=t dupedist=40 optical=t    # this will likely fail for remote (SRA) samples\n",
    "  fastqc_on_dedup: \"\"\n",
    "  # adapter removal & quality trimming/filtering\n",
    "  bbduk: ref=./adapters/bbmap_adapters.fa fastawrap=300 k=23\n",
    "  skewer: -x ./adapters/PE_all.fa -n -l 100 -q 25\n",
    "  fastqc_on_qual: \"\"\n",
    "  # removal of 'contaminant' reads\n",
    "  bbmap: minratio=0.9 maxindel=1 bwr=0.16 bw=12 fast minhits=2 qtrim=r trimq=10 untrim idtag printunmappedcount kfilter=25 maxsites=1 k=14 pairlen=1000 rescuedist=1000\n",
    "  fastqc_on_filter: \"\"\n",
    "  fastqc_on_final: \"\"\n",
    "  # taxonomy\n",
    "  centrifuge: Skip\n",
    "  krona: Skip\n",
    "  # coverage\n",
    "  nonpareil: -T kmer\n",
    "  nonpareil_summary: 1e9   # this is target seq. depth\n",
    "  # master \"Skip\": reads combined then called \"final\" reads (skips all QC steps)\n",
    "  skip_all_QC: False\n",
    "\n",
    "#-- Databases --#\n",
    "## hg19 = human genome database for filtering out human reads\n",
    "filter_db: /ebio/abt3_projects2/databases_no-backup/hg19/hg19\n",
    "# centrifuge db\n",
    "centrifuge_db: /ebio/abt3_projects2/databases_no-backup/centrifuge/p+h+v\n",
    "# krona taxonomy db\n",
    "krona_tax_db: /ebio/abt3_projects2/databases_no-backup/krona/taxonomy\n",
    "\n",
    "\n",
    "#-- Snakemake pipeline --#\n",
    "## To use /tmp/global2/, see http://ilm.eb.local/user-guide/#Scratch-space-on-_002ftmp_002fglobal2\n",
    "pipeline:\n",
    "  snakemake_folder: ./\n",
    "  script_folder: ./bin/scripts/\n",
    "  temp_folder: /tmp/global/    # your username will be added automatically to this path\n",
    "  run_skip_locally: True        # trivial \"skip\" steps run locally (not qsub)\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-05-08T13:23:20.256163Z",
     "start_time": "2019-05-08T13:23:20.249730Z"
    }
   },
   "outputs": [],
   "source": [
    "# Create output directory\n",
    "cami_qc_output = os.path.join(work_dir, \"data\", \"qc_cami\")\n",
    "if not os.path.exists(cami_qc_output):\n",
    "    os.mkdir(cami_qc_output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-05-08T13:23:20.900223Z",
     "start_time": "2019-05-08T13:23:20.772154Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "#-- I/O --#\r\n",
      "# table with sample --> read_file information\r\n",
      "samples_file: /ebio/abt3_projects/small_projects/jdelacuesta/DBs_benchmark/data/cami/samples.txt\r\n",
      "\r\n",
      "# output location\r\n",
      "output_dir: /ebio/abt3_projects/small_projects/jdelacuesta/DBs_benchmark/data/qc_cami\r\n",
      "\r\n",
      "# read file path\r\n",
      "# use \"None\" if full file path is included in the samples_file\r\n",
      "read_file_path: None\r\n",
      "\r\n",
      "#-- Software parameters --#\r\n",
      "# Use \"Skip\" to skip any of these steps. If no params for rule, use \"\"\r\n",
      "# `clumpify`: change dupedist if not HiSeq3000/4000 (dupedist=40 for NextSeq, HiSeq2500, and MiSeq)\r\n",
      "params:\r\n",
      "  # read download\r\n",
      "  remote: just_single=False      # If True, just download read1 (if remote file)\r\n",
      "  # validation, conversion, subsampling\r\n",
      "  validate_reads: Skip # \"\"\r\n",
      "  convert_fastq_to_1.8: \"\"\r\n",
      "  seqtk_sample: Skip    # Use number to subsample reads (eg., 1000000)\r\n",
      "  fastqc_on_raw: \"\"\r\n",
      "  # de-duplication\r\n",
      "  clumpify: Skip # dedupe=t dupedist=40 optical=t    # this will likely fail for remote (SRA) samples\r\n",
      "  fastqc_on_dedup: \"\"\r\n",
      "  # adapter removal & quality trimming/filtering\r\n",
      "  bbduk: ref=./adapters/bbmap_adapters.fa fastawrap=300 k=23\r\n",
      "  skewer: -x ./adapters/PE_all.fa -n -l 100 -q 25\r\n",
      "  fastqc_on_qual: \"\"\r\n",
      "  # removal of 'contaminant' reads\r\n",
      "  bbmap: minratio=0.9 maxindel=1 bwr=0.16 bw=12 fast minhits=2 qtrim=r trimq=10 untrim idtag printunmappedcount kfilter=25 maxsites=1 k=14 pairlen=1000 rescuedist=1000\r\n",
      "  fastqc_on_filter: \"\"\r\n",
      "  fastqc_on_final: \"\"\r\n",
      "  # taxonomy\r\n",
      "  centrifuge: Skip\r\n",
      "  krona: Skip\r\n",
      "  # coverage\r\n",
      "  nonpareil: -T kmer\r\n",
      "  nonpareil_summary: 1e9   # this is target seq. depth\r\n",
      "  # master \"Skip\": reads combined then called \"final\" reads (skips all QC steps)\r\n",
      "  skip_all_QC: False\r\n",
      "\r\n",
      "#-- Databases --#\r\n",
      "## hg19 = human genome database for filtering out human reads\r\n",
      "filter_db: /ebio/abt3_projects2/databases_no-backup/hg19/hg19\r\n",
      "# centrifuge db\r\n",
      "centrifuge_db: /ebio/abt3_projects2/databases_no-backup/centrifuge/p+h+v\r\n",
      "# krona taxonomy db\r\n",
      "krona_tax_db: /ebio/abt3_projects2/databases_no-backup/krona/taxonomy\r\n",
      "\r\n",
      "\r\n",
      "#-- Snakemake pipeline --#\r\n",
      "## To use /tmp/global2/, see http://ilm.eb.local/user-guide/#Scratch-space-on-_002ftmp_002fglobal2\r\n",
      "pipeline:\r\n",
      "  snakemake_folder: ./\r\n",
      "  script_folder: ./bin/scripts/\r\n",
      "  temp_folder: /tmp/global/    # your username will be added automatically to this path\r\n",
      "  run_skip_locally: True        # trivial \"skip\" steps run locally (not qsub)\r\n"
     ]
    }
   ],
   "source": [
    "config = config_text.format(samples_file=samples_file, \n",
    "                       output_dir=cami_qc_output)\n",
    "config_file_new = os.path.join(pipeline_folder, 'config_cami.yaml')\n",
    "with open(config_file_new, 'w') as outF:\n",
    "    outF.write(config)\n",
    "\n",
    "!cat $config_file_new "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-05-08T13:23:21.264227Z",
     "start_time": "2019-05-08T13:23:21.256063Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cd /ebio/abt3_projects/small_projects/jdelacuesta/DBs_benchmark/bin/llmgqc; source activate snakemake; screen -L -S llmgqc ./snakemake_sge.sh config_cami.yaml cluster.json /ebio/abt3_projects/small_projects/jdelacuesta/DBs_benchmark/tmp/SGE_out/llmgqc 10     --keep-going --rerun-incomplete --dryrun\n"
     ]
    }
   ],
   "source": [
    "# Create snakemake command\n",
    "conda_env = 'source activate snakemake'\n",
    "SGE_out_dir = os.path.join(work_dir, \"tmp/SGE_out/llmgqc\")\n",
    "QC_cmd = \"cd {llmgqc}; {conda_env}; screen -L -S llmgqc {exe} config_cami.yaml cluster.json {SGE_out} {jobs} \\\n",
    "    --keep-going --rerun-incomplete --dryrun\"\n",
    "\n",
    "QC_cmd = QC_cmd.format(conda_env = conda_env,\n",
    "                 llmgqc = pipeline_folder, \n",
    "                 exe = './snakemake_sge.sh',\n",
    "                 SGE_out = SGE_out_dir,\n",
    "                 jobs = 10)\n",
    "print(QC_cmd)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Session Info"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-09-11T08:14:28.348342Z",
     "start_time": "2019-09-11T08:14:28.216071Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "==> /ebio/abt3_projects/small_projects/jdelacuesta/DBs_benchmark/bin/llmgqc/bin/envs/bbmap.yaml <==\r\n",
      "channels:\r\n",
      "- conda-forge\r\n",
      "- bioconda\r\n",
      "dependencies:\r\n",
      "- pigz\r\n",
      "- bioconda::bbmap=37.78\r\n",
      "\r\n",
      "==> /ebio/abt3_projects/small_projects/jdelacuesta/DBs_benchmark/bin/llmgqc/bin/envs/centrifuge.yaml <==\r\n",
      "channels:\r\n",
      "- conda-forge\r\n",
      "- bioconda\r\n",
      "dependencies:\r\n",
      "- pigz\r\n",
      "- bioconda::centrifuge\r\n",
      "\r\n",
      "==> /ebio/abt3_projects/small_projects/jdelacuesta/DBs_benchmark/bin/llmgqc/bin/envs/fastqc.yaml <==\r\n",
      "channels: !!python/tuple\r\n",
      "- bioconda\r\n",
      "dependencies:\r\n",
      "- bioconda::fastqc=0.11.7\r\n",
      "\r\n",
      "==> /ebio/abt3_projects/small_projects/jdelacuesta/DBs_benchmark/bin/llmgqc/bin/envs/fqtools.yaml <==\r\n",
      "channels:\r\n",
      "- conda-forge\r\n",
      "- bioconda\r\n",
      "dependencies:\r\n",
      "- pigz\r\n",
      "- python=3\r\n",
      "- bioconda::seqtk\r\n",
      "- bioconda::biopython=1.70\r\n",
      "- bioconda::fqtools=2.0\r\n",
      "==> /ebio/abt3_projects/small_projects/jdelacuesta/DBs_benchmark/bin/llmgqc/bin/envs/krona.yaml <==\r\n",
      "channels:\r\n",
      "- conda-forge\r\n",
      "- bioconda\r\n",
      "dependencies:\r\n",
      "- pigz\r\n",
      "- bioconda::krona\r\n",
      "==> /ebio/abt3_projects/small_projects/jdelacuesta/DBs_benchmark/bin/llmgqc/bin/envs/multiqc.yaml <==\r\n",
      "channels:\r\n",
      "- conda-forge\r\n",
      "- bioconda\r\n",
      "dependencies:\r\n",
      "- bioconda::multiqc=1.5a\r\n",
      "\r\n",
      "==> /ebio/abt3_projects/small_projects/jdelacuesta/DBs_benchmark/bin/llmgqc/bin/envs/nonpareil.yaml <==\r\n",
      "channels:\r\n",
      "- conda-forge\r\n",
      "- bioconda\r\n",
      "dependencies:\r\n",
      "- pigz\r\n",
      "- bioconda::nonpareil\r\n",
      "\r\n",
      "==> /ebio/abt3_projects/small_projects/jdelacuesta/DBs_benchmark/bin/llmgqc/bin/envs/remote.yaml <==\r\n",
      "channels:\r\n",
      "- conda-forge\r\n",
      "- bioconda\r\n",
      "dependencies:\r\n",
      "- pigz\r\n",
      "- bioconda::sra-tools=2.8.2-0\r\n",
      "- bioconda::parallel-fastq-dump\r\n",
      "\r\n",
      "==> /ebio/abt3_projects/small_projects/jdelacuesta/DBs_benchmark/bin/llmgqc/bin/envs/skewer.yaml <==\r\n",
      "channels:\r\n",
      "- conda-forge\r\n",
      "- bioconda\r\n",
      "dependencies:\r\n",
      "- pigz\r\n",
      "- bioconda::skewer=0.2.2\r\n"
     ]
    }
   ],
   "source": [
    "sessionInfo = \"find {0} -name '*.yaml' | xargs head -n 1000\".format(os.path.join(pipeline_folder, 'bin', 'envs'))\n",
    "!$sessionInfo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:py3]",
   "language": "python",
   "name": "conda-env-py3-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.6"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
